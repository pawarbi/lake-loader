/*
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package ai.onehouse.lakeloader

import ai.onehouse.lakeloader.ChangeDataGenerator.{COMPRESSION_RATIO_GUESS, KeyTypes, UpdatePatterns, genParallelRDD}
import ai.onehouse.lakeloader.ChangeDataGenerator.KeyTypes.KeyType
import ai.onehouse.lakeloader.ChangeDataGenerator.UpdatePatterns.{Uniform, UpdatePatterns, Zipf}
import org.apache.hadoop.fs.Path
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.CatalystUtil.partitionLocalLimit
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.{DataFrame, Row, SaveMode, SparkSession}
import ai.onehouse.lakeloader.utils.StringUtils.lineSepBold
import ai.onehouse.lakeloader.utils.{MathUtils, StringUtils}

import java.io.Serializable
import java.time.LocalDate
import java.util.UUID.randomUUID
import scala.util.Random

/**
 * Class that can generates the workload based on advanced criteria like insert vs update ratios & update
 * and insert patterns (spread across partitions).
 *
 * @param spark     Spark's session
 * @param numRounds number of runs of workload generation and the measured operation
 */
class ChangeDataGenerator(val spark: SparkSession, val numRounds: Int = 10) extends Serializable {

  private val SEED: Long = 378294793957830L
  private val random = new Random(SEED)

  import spark.implicits._

  // Currently only supports flat schema.
  private def getSchema(numFields: Int = 10): StructType = {
    // First 4 fields are fixed: primary key, partition key, round id, and timestamp.
    val fields = Seq(
      StructField("key", StringType, nullable = false),
      StructField("partition", StringType, nullable = false),
      StructField("round", IntegerType, nullable = false),
      StructField("ts", LongType, nullable = false),
    ) ++ (0 until numFields - 4)
      .map(i => {
        i % 10 match {
          case 0 => StructField(s"textField1$i", StringType, nullable = true)
          case 1 => StructField(s"textField2$i", StringType, nullable = true)
          case 2 => StructField(s"textField3$i", StringType, nullable = true)
          case 3 => StructField(s"textField4$i", StringType, nullable = true)
          case 4 => StructField(s"textField5$i", StringType, nullable = true)
          case 5 => StructField(s"longField1$i", LongType, nullable = true)
          case 6 => StructField(s"decimalField$i", FloatType, nullable = true)
          case 7 => StructField(s"longField2$i", LongType, nullable = true)
          case 8 => StructField(s"longField3$i", LongType, nullable = true)
          case 9 => StructField(s"intField1$i", IntegerType, nullable = true)
        }
      })
    StructType(fields)
  }

  private def generateNewRecord(round: Int,
                                size: Int,
                                partitionPaths: List[String],
                                partitionDistributionCDF: List[Double],
                                keyType: KeyType,
                                schema: StructType) = {
    val ts = System.currentTimeMillis()
    // To induce (semi-) ordering on the keys we simply prefix its random part (UUID) w/ a ts;
    //
    // NOTE: That even though round is preceding the timestamp, it's a constant value for all
    //       records in a batch and therefore doesn't affect the ordering
    val key = keyType match {
      case KeyTypes.TemporallyOrdered =>
        s"${ts}-${randomUUID()}-${"%03d".format(round)}"
      case KeyTypes.Random =>
        s"${randomUUID()}-${"%03d".format(round)}"
      case _ => throw new UnsupportedOperationException(s"$keyType not supported")
    }

    val sizeFactor: Int = Math.max(size / schema.fields.size, 1)
    val fieldValues = schema.fields.map(
      field => {
        field.name match {
          case "key" => key
          case "partition" => partitionPaths(MathUtils.sampleFromCDF(partitionDistributionCDF, random.nextDouble()))
          case "round" => round
          case "ts" => ts
          case randomField => if (randomField.startsWith("textField")) {
            StringUtils.generateRandomString(sizeFactor + random.nextInt(sizeFactor))
          } else if (randomField.startsWith("decimalField")) {
            random.nextFloat()
          } else if (randomField.startsWith("longField")) {
            random.nextLong()
          } else if (randomField.startsWith("intField")) {
            random.nextInt()
          } else if (randomField.startsWith("arrayField")) {
            (0 until size / sizeFactor / 5).toArray
          } else if (randomField.startsWith("mapField")) {
            (0 until size / sizeFactor / 2 / 40).map(_ => (randomUUID(), random.nextInt())).toMap
          } else {
            throw new IllegalArgumentException(s"${field.name} not defined in schema.")
          }
        }
      })

    Row.fromSeq(fieldValues)
  }

  /**
   * Executes the spark DAG to generate the workload ahead of time, for the configured number of rounds.
   *
   * @param path                           path to place generated input local_workloads at
   * @param roundsDistribution             total number of records to generate per round
   * @param numColumns                      total number of columns in the schema
   * @param recordSize                     size of each record in bytes
   * @param updateRatio                    ratio of updates in the batch (remaining will be inserts)
   * @param totalPartitions                Number of total partitions
   * @param partitionDistributionMatrixOpt defines to-be-generated new records' distribution across partitions (for every round)
   * @param targetDataFileSize             data file size hint that data generation will aim to produce
   * @param skipIfExists                   should skip generation for the rounds possibly generated during previous
   * @param keyType                        format for generating the primary key
   * @param startRound                     round to start generating from, default 0.
   * @param updatePatterns                 Update pattern for generating updates: random (uniform) or zipf (skewed).
   * @param numPartitionsToUpdate          Number of partitions to update (default 1)
   */
  def generateWorkload(path: String,
                       roundsDistribution: List[Long] = List.fill(numRounds)(1000000L),
                       numColumns: Int = 10,
                       recordSize: Int = 1024,
                       updateRatio: Double = 0.5f,
                       totalPartitions: Int = 1,
                       partitionDistributionMatrixOpt: Option[List[List[Double]]] = None,
                       targetDataFileSize: Int = 128 * 1024 * 1024,
                       skipIfExists: Boolean = false,
                       keyType: KeyType = KeyTypes.Random,
                       startRound: Int = 0,
                       updatePatterns: UpdatePatterns = UpdatePatterns.Uniform,
                       numPartitionsToUpdate: Int = 1): Unit = {
    assert(totalPartitions != -1 || partitionDistributionMatrixOpt.isDefined)
    assert(numColumns > 5, "The number of columns needs to be above 5 since we need at least 4 cols for key, partition, round, and timestamp.")
    assert(numPartitionsToUpdate <= totalPartitions, "the number of partitions to update should be lower than the total partitions")

    // Compute records distribution matrix across partitions; such matrix
    // could be explicitly provided as an optional parameter prescribing corresponding
    // distribution for every round
    val (targetPartitionsCount, computedPartitionDistMatrix) =
      genPartitionsDistributionMatrix(totalPartitions, partitionDistributionMatrixOpt)

    val partitionPaths = genDateBasedPartitionValues(targetPartitionsCount)
    val schema = getSchema(numColumns)

    ////////////////////////////////////////
    // Generating workload
    ////////////////////////////////////////

    (startRound until numRounds).foreach(curRound => {
      val targetLocation = s"$path/$curRound"
      val partitionDistribution = computedPartitionDistMatrix(curRound)
      // Compute CDF for corresponding records distribution across partitions (for subsequent sampling)
      val partitionDistributionCDF = MathUtils.makeCDF(partitionDistribution)

      val targetLocationPath = new Path(targetLocation)
      val fs = targetLocationPath.getFileSystem(spark.sparkContext.hadoopConfiguration)

      if (skipIfExists && fs.exists(targetLocationPath) && fs.listFiles(targetLocationPath, false).hasNext) {
        println(s"Skipping generation for round # $curRound, location $targetLocation is not empty")
      } else {
        // Calculate inserts/updates split
        val targetRecords = roundsDistribution(curRound)
        val numUpdates = if (curRound == 0) 0 else Math.min((updateRatio * targetRecords).toLong, curRound * targetRecords)
        val numInserts = targetRecords - numUpdates

        val targetParallelism = Math.max(2, (targetRecords / (targetDataFileSize / (recordSize * COMPRESSION_RATIO_GUESS))).toInt)

        println(
          s"""
             |$lineSepBold
             |Round # $curRound: numInserts $numInserts, numUpdates $numUpdates
             |Creating at $targetLocation
             |$lineSepBold
             |""".stripMargin)

        ////////////////////////////////////////
        // Generating updates based on distribution type.
        ////////////////////////////////////////
        val rawUpdatesDF = updatePatterns match {
          case Uniform =>
            getRandomlyDistributedUpdates(partitionPaths, numUpdates, numPartitionsToUpdate, path, curRound)
          case Zipf =>
            getZipfDistributedUpdates(partitionPaths, numUpdates, numPartitionsToUpdate, path, curRound)
          case _ =>
            throw new IllegalArgumentException(s"Unsupported update pattern: $updatePatterns")
        }

        val newTs = System.currentTimeMillis()
        // Update the timestamp and current round for the updated record.
        val finalUpdatedDf = rawUpdatesDF
          .withColumn("ts", lit(newTs))
          .withColumn("round", lit(curRound))

        // NOTE: Applying this limit does not guarantee that exactly N elements will be contained in the
        //       returned dataset, since it might not be applying Spark's [[GlobalLimit]] operator.
        //       Instead, it might return slightly higher number of the records (but no more than O(number of partitions)),
        //       since we're simply applying [[LocalLimit]] to circumvent the performance implications of
        //       [[GlobalLimit]] for very large datasets (coalescing all partitions into a single one, then doing
        //       a limit on it)
        val updatesDF = partitionLocalLimit(finalUpdatedDf.repartition(targetParallelism), numUpdates.toInt)

        ////////////////////////////////////////
        // Generating inserts
        ////////////////////////////////////////

        val insertsRDD = genParallelRDD(spark, targetParallelism, 0, numInserts)
          .map(_ => generateNewRecord(curRound, recordSize, partitionPaths, partitionDistributionCDF, keyType, schema))

        val insertsDF = spark.createDataFrame(insertsRDD, schema)
        val upsertDF = if (numUpdates == 0) insertsDF else insertsDF.union(updatesDF)

        spark.time {
          upsertDF
            .repartition(targetParallelism)
            .write
            .format(ChangeDataGenerator.DEFAULT_DATA_GEN_FORMAT)
            .mode(SaveMode.Overwrite)
            .save(targetLocation)
        }

        spark.catalog.clearCache()
      }
    })
  }

  private def getZipfDistributedUpdates(partitionPaths: List[String],
                                        numUpdateRecords: Long,
                                        numPartitionsToWrite: Int,
                                        path: String,
                                        currentRound: Int): DataFrame = {
    val numRecordsPerPartition: List[Int] = MathUtils.zipfDistribution(numUpdateRecords, numPartitionsToWrite)
    val partitionsToUpdate = partitionPaths.take(numPartitionsToWrite)
    println(s"Generating zipf distributed updates from partitions for round # $currentRound: Partitions $partitionsToUpdate")

    var sourceDf = spark.read.format(ChangeDataGenerator.DEFAULT_DATA_GEN_FORMAT).load(s"$path/*")
    sourceDf = sourceDf.filter(col("partition").isin(partitionsToUpdate: _*))
    val updateCount = sourceDf.count()
    println(s"Number of updates total records for round # $currentRound: COUNT $updateCount")

    sourceDf.createOrReplaceTempView("source_df_partitions")

    var rankedDF = spark.sql(
      """
        | SELECT key, partition, `round`, rank(key) OVER (PARTITION BY key ORDER BY round DESC) as key_rank
        | FROM source_df_partitions
        |""".stripMargin
    )
    rankedDF = rankedDF.filter($"key_rank" === 1).drop(s"key_rank")
    rankedDF.persist()

    val partitionCounts: Map[String, Long] = rankedDF
      .groupBy("partition")
      .count()
      .collect()
      .map(row => row.getAs[String]("partition") -> row.getAs[Long]("count"))
      .toMap

    val samplingRatios: Map[String, Double] = partitionCounts.map {
      case (partition, totalRecords) =>
        val desiredCount = numRecordsPerPartition(partitionPaths.indexOf(partition))
        val ratio = 1.0 * desiredCount.toDouble / totalRecords.toDouble
        partition -> ratio
    }

    var fullPlan = spark.emptyDataFrame
    var count: Int = 0
    samplingRatios.foreach(x => {
      val ppf = rankedDF.filter($"partition" === x._1).sample(x._2)
      ppf.persist()
      fullPlan = if (count == 0) {
        count = 1
        ppf
      } else {
        fullPlan.union(ppf)
      }
    })

    // Join sourceDf with fullPlan on key, partition, and round
    val joinCols = Seq("key", "partition", "round")
    val joinedDf = sourceDf.join(fullPlan, joinCols, "inner")
    joinedDf
  }

  private def getRandomlyDistributedUpdates(partitionPaths: List[String],
                                            numUpdateRecords: Long,
                                            numPartitionsToWrite: Int,
                                            path: String,
                                            currentRound: Int): DataFrame = {
    val partitionsToUpdate = partitionPaths.take(numPartitionsToWrite)
    println(s"Generating random updates from partitions for round # $currentRound: Partitions $partitionsToUpdate")

    var sourceDf = spark.read.format(ChangeDataGenerator.DEFAULT_DATA_GEN_FORMAT).load(s"$path/*")
    sourceDf = sourceDf.filter(col("partition").isin(partitionsToUpdate: _*))
    sourceDf.persist()
    val totalRecords = sourceDf.count()
    println(s"Number of updates total records for round # $currentRound: COUNT $totalRecords")

    val samplingRatio = 1.0 * numUpdateRecords.toDouble / totalRecords.toDouble
    val finalDF = sourceDf
      // NOTE: We should not be filtering out records as it will reduce number of updated records we sample
      .sample(samplingRatio)
    finalDF
  }

  private def genDateBasedPartitionValues(targetPartitionsCount: Int): List[String] = {
    // This will generate an ordered sequence of dates in the format of "yyyy/mm/dd"
    // (where most recent one is the first element)
    List.fill(targetPartitionsCount)(LocalDate.now()).zipWithIndex
      .map(t => t._1.minusDays(targetPartitionsCount - t._2))
      .map(d => s"${d.getYear}-${"%02d".format(d.getMonthValue)}-${"%02d".format(d.getDayOfMonth)}")
      .reverse
  }

  private def genPartitionsDistributionMatrix(totalPartitions: Int, partitionDistributionMatrixOpt: Option[List[List[Double]]]) = {
    partitionDistributionMatrixOpt match {
      case Some(partitionDistMatrix) =>
        assert(partitionDistMatrix.size == numRounds)
        partitionDistMatrix.foreach { dist =>
          assert(totalPartitions == -1 || totalPartitions == dist.size, s"$totalPartitions != ${dist.size}")
          assert((dist.sum - 1.0) < 1e-5, s"${dist.sum} != 1.0")
        }

        (partitionDistMatrix.head.size, partitionDistMatrix)

      case None =>
        val dist = List.fill(totalPartitions)(1.0 / totalPartitions)

        (dist.size, List.fill(numRounds)(dist))
    }
  }
}

case class DatagenConfig(outputPath: String = "",
                         numberOfRounds: Int = 10,
                         numberRecordsPerRound: Long = 1000000,
                         numberColumns: Int = 10,
                         recordSize: Int = 1024,
                         updateRatio: Double = 0.5f,
                         totalPartitions: Int = 1,
                         targetDataFileSize: Int = 128 * 1024 * 1024,
                         skipIfExists: Boolean = false,
                         startRound: Int = 0,
                         keyType: KeyType = KeyTypes.Random,
                         updatePattern: UpdatePatterns = UpdatePatterns.Uniform,
                         numPartitionsToUpdate: Int = 20
                 )

object ChangeDataGenerator {

  val COMPRESSION_RATIO_GUESS = .66
  val DEFAULT_DATA_GEN_FORMAT: String = "parquet"

  object KeyTypes extends Enumeration {
    type KeyType = Value
    val Random, TemporallyOrdered = Value
  }

  object UpdatePatterns extends Enumeration {
    type UpdatePatterns = Value
    val Uniform, Zipf = Value
  }

  def main(args: Array[String]): Unit = {

    implicit val keyTypeRead: scopt.Read[KeyType] = scopt.Read.reads { s =>
      try {
        KeyTypes.withName(s)
      } catch {
        case _: NoSuchElementException =>
          throw new IllegalArgumentException(s"Invalid key type: $s. Valid values: ${KeyTypes.values.mkString(", ")}")
      }
    }

    implicit val updatePatternsRead: scopt.Read[UpdatePatterns] = scopt.Read.reads { s =>
      try {
        UpdatePatterns.withName(s)
      } catch {
        case _: NoSuchElementException =>
          throw new IllegalArgumentException(s"Invalid update pattern: $s. Valid values: ${UpdatePatterns.values.mkString(", ")}")
      }
    }

    val parser = new scopt.OptionParser[DatagenConfig]("lake-loader | change data generator") {
      head("change data generator usage")

      opt[String]('p', "path")
        .required()
        .action((x, c) => c.copy(outputPath = x))
        .text("Input path")

      opt[Int]("number-rounds")
        .action((x, c) => c.copy(numberOfRounds = x))
        .text("Number of rounds of incremental change data to generate. Default 10.")

      opt[Long]("number-records-per-round")
        .action((x, c) => c.copy(numberRecordsPerRound = x))
        .text("Number of columns in schema of generated data. Default: 1000000.")

      opt[Int]("number-columns")
        .action((x, c) => c.copy(numberColumns = x))
        .text("Number of columns in schema of generated data. Default: 10, minimum 5.")

      opt[Int]("record-size")
        .action((x, c) => c.copy(recordSize = x))
        .text("Record Size of the generated data.")

      opt[Double]("update-ratio")
        .action((x, c) => c.copy(updateRatio = x))
        .text("Ratio of updates to total records generated in each incremental batch")

      opt[Int]("total-partitions")
        .action((x, c) => c.copy(totalPartitions = x))
        .text("Total number of partitions desired for the benchmark table.")

      opt[Int]("datagen-file-size")
        .action((x, c) => c.copy(targetDataFileSize = x))
        .text("Target data file size for the data generated files.")

      opt[Boolean]("skip-if-exists")
        .action((x, c) => c.copy(skipIfExists = x))
        .text("Skip generated data if folder already exists.")

      opt[Int]("starting-round")
        .action((x, c) => c.copy(startRound = x))
        .text("Generate data from specified round. default: 0")

      opt[UpdatePatterns]("update-pattern")
        .action((x, c) => c.copy(updatePattern = x))
        .text("The pattern for the updates to be generated for the data.")

      opt[KeyType]("primary-key-type")
        .action((x, c) => c.copy(keyType = x))
        .text(s"Primary key type for generated data. Options: ${KeyTypes.values.mkString(", ")}")

      opt[Int]("numPartitionsToUpdate")
        .action((x, c) => c.copy(numPartitionsToUpdate = x))
        .text("Number of partitions that should have at least 1 records written to.")
    }

    parser.parse(args, DatagenConfig()) match {
      case Some(config) =>
        val spark = SparkSession.builder
          .appName("ChangeDataGeneratorApp")
          .getOrCreate()
        val changeDataGenerator = new ChangeDataGenerator(spark, config.numberOfRounds)
        changeDataGenerator.generateWorkload(config.outputPath,
          List.fill(config.numberOfRounds)(config.numberRecordsPerRound),
          config.numberColumns,
          config.recordSize,
          config.updateRatio,
          config.totalPartitions,
          None,
          config.targetDataFileSize,
          config.skipIfExists,
          config.keyType,
          config.startRound,
          config.updatePattern,
          config.numPartitionsToUpdate
        )

        spark.stop()

      case None =>
        // scopt already prints help
        sys.exit(1)
    }
  }

  private def genParallelRDD(spark: SparkSession, targetParallelism: Int, start: Long, end: Long): RDD[Long] = {
    val partitionSize = (end - start) / targetParallelism
    spark.sparkContext.parallelize(0 to targetParallelism, targetParallelism)
      .mapPartitions { it =>
        val partitionStart = it.next() * partitionSize
        (partitionStart to partitionStart + partitionSize).iterator
      }
  }
}
